\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}        % so we can use the 'pretty' empty set
\usepackage{tipa}
\usepackage{graphicx}           % purdy pitchers
\usepackage{algorithmic}
\usepackage{phonrule}

\usepackage{cite}
\usepackage{apacite}

\newtheorem{definition}{Definition}

\title{An algorithm to assign features to a set of phonological classes}
\author{}
\author{
  Mayer, Connor \\
  \texttt{connormayer@ucla.edu}
  \and
  Daland, Robert \\
  \texttt{r.daland@gmail.com}
}
\date{\vspace{-5ex}}							% Activate to display a given date or no date

\begin{document}
\maketitle

\begin{abstract}
This squib describes a dynamic programming algorithm which assigns features to a set of phonological classes. The input consists of a set of classes, each containing one or more segments; in other words, a subset of the powerset of a segmental alphabet $\Sigma$. If a class can be generated as the union of existing features ( = intersection of already-processed classes), those features are propagated to every segment in the class. Otherwise, a new feature/value pair is assigned. The algorithm comes in 4 flavors, which differ with respect to complementation and how negative values are assigned. We show that these variants yield \textit{privative specification}, \textit{contrastive underspecification}, \textit{contrastive specification}, and \textit{full specification}, respectively. The main text sets out necessary background, and illustrates each variant of the algorithm. The Appendix formally proves that each algorithm is sound.
\end{abstract}

\section{Introduction}
Distinctive features are the building blocks of phonological theory. Features represent phonetic qualities of speech sounds, and can be used in isolation or combination to describe individual sounds or classes of sounds \cite<e.g.,>{Saussure1959, JakobsonEtAl1952}. This captures the generalization that sounds with similar phonetic properties tend to pattern similarly in the phonologies of languages. For example, the feature \phonfeat{-voice} picks out all sounds without voicing, while the combination of features \phonfeat{-voice \\ -continuant} picks out, for example, the English voiceless stop series.

Distinctive features have traditionally been described as \textit{innate}: that is, all the sounds in the world's languages can be described by the same finite set of features, and the task of the language learner is to decompose the sounds of the ambient language into their consituent features to build a phonological grammar \cite<e.g.,>{ChomskyHalle1968}. This implies that the phonological classes we see in natural languages should be definable by combinations of these innate features.

Although feature theory has been extremely productive in phonology, there is evidence that shows many phonological classes cannot be described as a combination of phonetically-based features. For example, \citeA{Mielke2008} conducted a survey of almost 600 languages and showed that, at best, any modern feature system can categorize 71\% of attested sound classes. The remaining 29\% are \textit{phonetically disparate classes}, which require less theoretically-appealing descriptions combinations of features to be described, such as \textsc{XOR} feature classes (e.g., \phonfeat{-voice} or \phonfeat{-continuant} but not both).

The preponderance of these phonetically disparate classes has led some researchers to propose that distinctive features are \textit{learned} and \textit{language-specific} \cite<e.g.,>{Blevins2004, Mielke2008, MacWhinneyOGrady2015, ArchangeliPulleyblank2015}: learners are able to group sounds in their languages into classes regardless of whether they have any phonetic commonality. The striking regularities that exist across languages are explained as by-products of general human cognitive capabilities, such as categorization, sensitivity to frequency, and the ability to generalize, as well as the properties of the human vocal tract and auditory system.

This sets the stage for the goals of the current paper, which are somewhat modest. The basic question we address is the inverse of how features have typically been approached: rather than asking what classes a feature system defines, we instead focus on how a feature system can be learned from a set of predetermined classes. We begin by defining a formal notation for feature systems. We then describe the \textit{intersectional closure} of a set of classes, which must be generated by any featurization of that set. Using the intersectional closure as a tool for efficient calculation, we then describe a suite of algorithms for learning various types of featurizations for a set of input classes and prove their correctness, examining as we do the trade offs between number of classes and number of features that each featurization method makes.

This paper makes several important contributions: first, it demonstrates a method for working backwards to feature systems underpinning learned classes of sounds and provides the code for use in future research. Second, it provides a detailed formalization of what a featurization of classes entails, allowing careful reasoning about the expressiveness of such featurizations. Finally, by comparing multiple types of featurization of a set of classes, it makes explicit predictions about what classes should be describable under each type, which may be useful for future experimental phonological research.

\section{Definitions and notation}

Let $\Sigma$ denote an alphabet of segments. We will use the term \textit{class} to mean a subset of $\Sigma$.

\subsection{Classes and class systems}

A \textit{(natural) class system} $(\mathcal C, \Sigma)$ consists of an alphabet $\Sigma$ and a set of classes $\mathcal C$ over that alphabet. Fig~\ref{fig:lattice} illustrates this definition with a natural class system over a set of vowels.

% show an example of a natural class system: a vowel harmony lattice
% TODO: it would be great to modify this figure so the 'stray' singletons on the right are on the same 'tier' as the other singletons
\begin{figure}[h]
\includegraphics[width=0.9\textwidth]{vowelHarmony_unicode.png}
\caption{Vowel harmony lattice}
\label{fig:lattice}
\end{figure}

In this graph, each node corresponds to a class, and a downward arrow from class $X$ to class $Y$ indicates that $Y \subset X$. However, the figure does not include an arrow for every pair of classes that have a subset relationship. For example, the class \{\textipa{E}\} is a subset of all vowels, but there is not an arrow from the class of all vowels to \{\textipa{E}\}. This is because the class of front vowels \{\textipa{\oe}, \textipa{y}, \textipa{E}, \textipa{i}\} `intervenes' between \{\textipa{E}\} and the class of all vowels. We will use the terms \textit{parent/daughter} to refer to cases in which a subset/superset relation holds and there is no intervening class.

Graphically, it is sufficient to indicate parent-daughter relations in figures like \ref{fig:lattice}, since other superset-subset relations (like \{\textipa{E}\} $\subset$ all vowels) are entailed. However, we did not only introduce this concept for aesthetically pleasing graphs. As we show later, the parenthood relation is essential for the featurization algorithm. Thus, we formalize the definition here: \begin{itemize}
    \item $X$ is a \textit{parent} of $Y$ (with respect to $\mathcal C$) if and only if $Y \subset X$, and $\nexists W \in \mathcal C \, [Y \subset W \subset X]$
    \end{itemize}

\noindent We further define $\textsc{Parents}(Y, \mathcal C)$ as the set of all parents of $Y$ (with respect to $\mathcal C$). 

\subsection{Feature systems and featurizations}

A \textit{feature system} is a tuple $(\mathcal F, \Sigma, \mathcal V)$ where \begin{itemize}
    \item $\Sigma$ is a segmental alphabet, 
    \item $\mathcal V$ is a set of values, and 
    \item $\mathcal F$ is a \textit{featurization}: a set of features $\{f_j\}_{j=1}^M$, where each feature is a function $f: \Sigma \rightarrow \mathcal V$ mapping segments to feature values
    \end{itemize}

\noindent To illustrate, a feature system for the vowel harmony lattice of Fig.~\ref{fig:lattice} is shown below in Table~\ref{table:featurization}. In the next subsection we formalize featural descriptors, which relate classes and feature systems.

\begin{table}[h]
    \centering
    \begin{tabular} {|c||c|c|c|c|c|}
    \hline
        $\sigma$ & front & back & low & high & round \\ \hline
        \textipa{i} & + & -- & -- & + & -- \\
        \textipa{y} & + & -- & -- & + & + \\
        \textipa{W} & -- & + & -- & + & -- \\
        \textipa{u} & -- & + & -- & + & + \\
        \textipa{E} & + & -- & -- & -- & -- \\
        \textipa{\oe} & + & -- & -- & -- & + \\
        \textipa{2} & -- & + & -- & -- & -- \\
        \textipa{O} & -- & + & -- & -- & + \\
        \textipa{a} & -- & + & + & -- & -- \\
        \hline
    \end{tabular}
    \caption{Example of a feature system.}
    \label{table:featurization}
\end{table}

\subsection{Featural descriptors}

Let $(\mathcal F, \Sigma, \mathcal V)$ be a feature system. We restrict $\mathcal V$ to the following possibilities: \begin{itemize}
    \item \textit{privative specification}: $\mathcal V = \{ +, 0 \}$
    \item \textit{full specification}: $\mathcal V = \{ +, - \}$
    \item \textit{contrastive specification}: $\mathcal V = \{ +, -, 0 \}$
    \end{itemize}

A \textit{featural descriptor} $\mathbf{e}$ is a set of feature/value pairs where the values cannot be $0$, i.e. $\mathbf{e} \subset (\mathcal V \setminus \{0\}) \times \mathcal F$. For example, $\mathbf{e} =$  \phonfeat{+front \\ -low} is a featural descriptor.

To relate featural descriptors and phonological classes, note that every featural descriptor $\mathbf{e}$ can be expressed in the form $\mathbf{e} = \{\alpha_k F_k\}_{k=1}^K$, where each $\alpha_k$ is a value in $\mathcal V \setminus \{ 0 \}$, and each $F_k$ is some feature function $f_j \in \mathcal F$. Informally, we say that a featural descriptor describes the class of segments which have (at least) the feature/value pairs it contains. Formally, we write $\langle \mathbf{e} \rangle$ to indicate the natural class that corresponds to the featural descriptor $\mathbf{e}$:

$$ \langle \, \{\alpha_k F_k\}_{k=1}^K \, \rangle = \{x \in \Sigma \, \mid \, F_k(x) = \alpha_k \text{ for every } k \} $$

\vspace{\baselineskip} \noindent We use the notation $\mathcal V^\mathcal F$ to denote the powerset of $(\mathcal V \setminus \{0\}) \times \mathcal F$, i.e. the set of all licit featural descriptors. Lastly, we define $\langle \mathcal V^\mathcal F \rangle = \{ \langle \mathbf{e} \rangle \, \mid \, \mathbf{e} \in \mathcal V^\mathcal F \}$, the set of all natural classes described by some featural descriptor in $\mathcal V^\mathcal F$. We say that the feature system $(\mathcal F, \Sigma, \mathcal V)$ generates the natural class system $\langle \mathcal V^\mathcal F \rangle$.

Note that while every featural descriptor in $\mathcal V^\mathcal F$ picks out a class in $\langle \mathcal V^\mathcal F \rangle$, the two are not in 1-1 correspondence. This is because the same class can often be described by multiple featural descriptors. For example, under the the feature system of Table~\ref{table:featurization}, the featural descriptor \phonfeat{+front} picks out the same class as the featural descriptor \phonfeat{+front \\ -low} (namely, the front vowels). Moreover, the featural descriptors \phonfeat{+front \\ -front} and \phonfeat{+high \\ +low} both pick out the empty set.

\vspace{\baselineskip} We say that a feature system $(\mathcal F, \Sigma, \mathcal V)$ \textit{covers} a natural class system $\mathcal C$ if $\mathcal C \subset \langle \mathcal V^\mathcal F \rangle$; in other words if the feature system provides a distinct featural representation for every class in $\mathcal C$. In the remainder of this squib, we show how to construct a feature system that covers an arbitrary natural class system $\mathcal C$.

We begin with a worked-out example illustrating the difference between privative and full specification with the same segmental alphabet. Then we introduce the notion of intersectional closure, which leads naturally to a featurization algorithm for privative specification. Simple modifications yield algorithms for contrastive and full specification.

\subsection{Example}

Let $\Sigma =$ \{R, D, T\}. Informally, the reader may think of [R] as a sonorant, [D] as a voiced obstruent, and [T] as a voiceless obstruent; accordingly we use the feature names $son$ and $vcd$. In this section, we illustrate the consequences of privative versus full specification, using featurizations that are isomorphic (that is, they match on the $+$ values, and differ only as to whether the non-$+$ values are $0$ or $-$). We begin with Table ~\ref{table:privative}.

% table with featurization of sonorants, voiced obstruents, and voiceless obstruents
\begin{table}[h]
    \centering
    \begin{tabular} {|c||c|c|}
    \hline
        $\sigma$ & son & vcd \\ \hline
        R & + & + \\
        D & 0 & + \\
        T & 0 & 0 \\
        \hline
    \end{tabular}
    \caption{Sonorants and obstruents with privative specification.}
    \label{table:privative}
\end{table}

\noindent The set of natural classes it describes, and the simplest featural descriptor for each, are shown below: \begin{itemize}
  \item $[\,]$ -- \{R, D, T\}
  \item $[+\text{son}]$ -- \{R\}
  \item $[+\text{vcd}]$ -- \{R, D\}
  \end{itemize}
  
\noindent Note that this featurization provides no featural descriptor that uniquely picks out the voiceless obstruent [T], no way to pick out the obstruents [T] and [D] to the exclusion of [R], and no way to pick out the voiced obstruent [D] without [R].

Next, consider the isomorphic featurization in which the $0$'s from Table ~\ref{table:privative} are replaced with $-$'s:

% table with featurization of sonorants, voiced obstruents, and voiceless obstruents
\begin{table}[h]
    \centering
    \begin{tabular} {|c||c|c|}
    \hline
        $\sigma$ & son & vcd \\ \hline
        R & + & + \\
        D & -- & + \\
        T & -- & -- \\
        \hline
    \end{tabular}
    \caption{Sonorants and obstruents with full specification.}
    \label{table:full}
\end{table}

\noindent The set of natural classes this featurization describes is much larger, because the number of (extensionally distinct) featural descriptors is larger: \begin{itemize}
    \item $[\,] =$ \{R, D, T\}
    \item $[+\text{son}] =$ \{R\}
    \item $[-\text{son}] =$ \{D, T\}
    \item $[+\text{vcd}] =$ \{R, D\}
    \item $[-\text{vcd}] =$ \{T\}
    \item $[-\text{son},+\text{vcd}] =$ \{D\}
    \item $[+\text{son},-\text{son}] = \varnothing$
    \end{itemize}

\noindent An important generalization emerges from comparing these featurizations: the more $0$'s in the featurization, the greater the number of distinct feature functions that will be required to cover the same natural class system. In one sense, privative specification is more complex, because it will normally involve more features. However, in another sense, it is simpler, because there are only $+$ values to handle and because it will result in fewer natural classes. Therefore, we will treat privative specification first. Prior to this, we introduce the notion of intersectional closure -- the data structure that proves useful for efficiently assigning a privative feature system.

\section{Intersectional closure}

In this section we define the \textit{intersectional closure} of a natural class system $\mathcal C$ as the set of classes that can be generated by intersecting $\Sigma$ with any set of classes in $\mathcal C$. We relate the intersectional closure to features by showing that if a feature system is expressive enough to generate all the classes in $\mathcal C$, it generates the intersectional closure. Then we give a dynamic programming algorithm which efficiently computes the intersectional closure. 

\subsection{Definition}

A collection of sets $\mathcal C$ is \textit{intersectionally closed} if and only if $\forall (X, Y \in \mathcal C) \, [ X \cap Y \in \mathcal C]$.

We write $\mathcal C_\cap$ to indicate the \textit{intersectional closure} of a natural class system $(\mathcal C, \Sigma)$. This the smallest intersectionally closed collection which contains $\Sigma$ and every set in $\mathcal C$; in other words, the intersectional closure does not contain any classes except $\Sigma$ and those which can be generated by finite intersections of classes from $\mathcal C$.

\subsection{Feature systems generate an intersectional closure}

Now we explain why any feature system that covers $\mathcal C$ must cover $\mathcal C_\cap$. The explanation rides on the dual relationship between featural descriptors and the classes they describe: the class described by the union of two featural descriptors is the intersection of the classes described by each of the descriptors alone. This principle can be illustrated with the following example, using the vowel system in Fig.~\ref{fig:lattice}. Let $\mathbf{e}_1 =$ \phonfeat{+front} and $\mathbf{e}_1 =$ \phonfeat{+round}. Then \begin{itemize}
    \item $\langle$\phonfeat{+front}$\rangle =$ \{\textipa{\oe}, \textipa{y}, \textipa{E}, \textipa{i}\}
    \item $\langle$\phonfeat{+round}$\rangle =$ \{\textipa{\oe}, \textipa{o}, \textipa{y}, \textipa{u}\}
    \item $\langle$\phonfeat{+front}$\rangle \cap \langle$\phonfeat{+round}$\rangle =$ \{\textipa{\oe}, \textipa{y}\}
    \item $\langle$\phonfeat{+front,+round}$\rangle =$ \{\textipa{\oe}, \textipa{y}\}
    \end{itemize}

In other words, the set of vowels that are both front and round is the intersection of the set of vowels that are front and the set of vowels that are round. Lemma 1 proves that this kind of relationship holds for any pair of featural descriptors (and the classes they describe).

\vspace{\baselineskip} \noindent \textbf{Lemma 1}: Let $(\mathcal F, \Sigma, \mathcal V)$ be a feature system. If $\mathbf{e}_i, \mathbf{e}_j \in \mathcal V^\mathcal F$, then $\langle \mathbf{e}_i \cup \mathbf{e}_j \rangle =  \langle \mathbf{e}_i \rangle \cap \langle \mathbf{e}_j \rangle$.

\vspace{\baselineskip} \noindent A formal proof is included in the Appendix. The key utility of this Lemma is that it can be applied inductively, to relate the union of multiple featural descriptors with the intersection of multiple classes.

\vspace{\baselineskip} \noindent \textbf{Theorem 1}: Let $(\mathcal C, \Sigma)$ be a natural class system and $(\mathcal F, \Sigma, \mathcal V)$ a feature set. If $\mathcal C \subset \langle \mathcal V^\mathcal F \rangle$, then $\mathcal C_\cap \subset\langle \mathcal V^\mathcal F \rangle $.

\textit{Proof}: Let $Y$ be an arbitrary class in $\mathcal C_\cap$. By definition of $\mathcal C_\cap$, there exist $\{X_i \in \mathcal C\}_{i \in I}$ (for some index set $I$, which we will omit hereafter) such that and $Y = \bigcap_i \, X_i$. The hypothesis that $\mathcal C \subset \langle \mathcal V^\mathcal F \rangle $ implies that for every such $X_i$, there exists a featural descriptor $\mathbf{e}_i$ such that $\langle \mathbf{e}_i \rangle = X_i$. Thus, $Y = \bigcap_i X_i = X_1 \cap X_2 \cap \ldots \cap X_n$ can also be written $C = \bigcap_i \, \langle \mathbf{e}_i \rangle = \langle \mathbf{e}_1 \rangle \cap \langle \mathbf{e}_2 \rangle \cap \ldots \cap \langle \mathbf{e}_n \rangle$. It follows by induction using Lemma 1 that $Y = \langle \bigcup_i \mathbf{e}_i \rangle$:

$Y = \langle \mathbf{e}_1 \rangle \cap  \langle \mathbf{e}_2 \rangle \cap \ldots \cap  \langle \mathbf{e}_n \rangle$

\quad $ = \langle \mathbf{e}_1 \cup \mathbf{e}_2 \rangle \cap \mathbf{e}_3 \cap \ldots \cap \langle \mathbf{e}_n \rangle$

\quad $ = \langle \mathbf{e}_1 \cup \mathbf{e}_2 \cup \mathbf{e}_3 \rangle \cap \ldots \cap \langle \mathbf{e}_n \rangle$

\quad $\ldots$

\quad $= \langle \mathbf{e}_1 \cup \mathbf{e}_2 \cup \ldots \cup \mathbf{e}_n \rangle$

\quad $= \langle \bigcup_i  \mathbf{e}_i \rangle$

\noindent The preceding chain of logic demonstrates that if a class can be expressed as the intersection of natural classes in $\mathcal C$, then its features are the union of the features in each of those classes. The intersectional closure is defined as all possible intersections of classes in $\mathcal C$. Thus, if $(\mathcal F, \Sigma, \mathcal V)$ covers $\mathcal C$, it covers the intersectional closure. This completes the proof.

\subsection{An algorithm for calculating the intersectional closure}

The following algorithm yields the intersectional closure of a natural class system $(\mathcal C, \Sigma)$. A proof by induction is given in the Appendix.

\noindent \begin{algorithmic}
    \ENSURE $\mathcal C_\cap$ is the intersectional closure of the input $\mathcal C$
    \STATE
    \STATE $\mathcal C_\cap \leftarrow \{ \Sigma \} $
    \STATE $\mathcal Q \leftarrow \mathcal C$
    \STATE
    \WHILE{$\mathcal Q \neq \varnothing$}
        \STATE $X \leftarrow \textsc{dequeue}(\mathcal Q)$
        \IF{\NOT $X \in \mathcal C_\cap$}
            \FOR{$Y \in \mathcal C_\cap$}
                \STATE $\textsc{enqueue}(\mathcal Q, X \cap Y)$
            \ENDFOR
            \STATE $\textsc{append}(\mathcal C_\cap, \, X)$
        \ENDIF
    \ENDWHILE
\end{algorithmic}

\subsection{Parenthood in the intersectional closure}

As we will see shortly, the advantage of explicitly computing the intersectional closure is that \textit{a new feature is required for all and only the classes which have a single parent} (in the intersectional closure). The core reason for this is that if a class has two parents, it must be their intersection. We prove this here.

\vspace{\baselineskip} \noindent \textbf{Theorem 2}: Let $(\mathcal C, \Sigma)$ be a natural class system and $Y \in \mathcal C_\cap$. Then $\textsc{Parents}(Y) = \{X_1, X_2\}$ implies $Y = X_1 \cap X_2$.

\textit{Proof}: First, we show $Y \subset X_1 \cap X_2$. This follows trivially from the definition of parenthood: $X_1$ is a parent of $Y$ implies $Y \subset X_1$, $X_2$ is a parent of $Y$ implies $Y \subset X_2$, and so every element in $Y$ is in both $X_1$ and $X_2$.

Next, we show that $X_1 \cap X_2 \subset Y$. Suppose not, i.e. $\exists x \in (X_1 \cap X_2) \setminus Y$. As we already showed, $Y \subset (X_1 \cap X_2)$, and $(X_1 \cap X_2) \in \mathcal C_\cap$. Of course $X_1 \cap X_2 \subset X_1$ follows from fundamental properties of sets, and $X_1 \in \mathcal C_\cap$ by hypothesis. These may be combined to yield $Y \subset (X_1 \cap X_2) \subset X_1$, so that $(X_1 \cap X_2)$ `intervenes' between $Y$ and $X_1$. But this implies that $Y$ cannot be a daughter of $X_1$, a contradiction. Thus, $X_1 \cap X_2 \subset Y$ and $Y \subset X_1 \cap X_2$, proving that $Y = X_1 \cap X_2$.
	
\vspace{\baselineskip} \noindent By very similar logic as in Theorem 2, it is straightforward to show that if a class in $\mathcal C_\cap$ has \textit{more} than 2 parents, then it must be the null set. It is similarly easy to show that $\Sigma$ is the only class in $\mathcal C_\cap$ with $0$ parents. Thus, every nonempty class in the intersectional closure of $\mathcal C$ besides $\Sigma$ has either 1 parent or 2, and in the latter case, it is the intersection of those two parents. We illustrate this point in the next subsection by by graphing the intersectional closure of the vowel system in Fig.~\ref{fig:lattice}.

\subsection{Illustration of the intersectional closure}

TODO: add figure

\section{Privative specification}
achieved by assigning a new feature [+f] to every segment in $X$

\vspace{\baselineskip} \noindent \begin{algorithmic}
	\REQUIRE $\mathcal V = \{ +, 0 \}$
	\REQUIRE Precompute $C_\cap \leftarrow \textsc{IntersectionalClosure}(C)$
	\ENSURE Output featurization $\mathcal F = \{ f_j \}_{j=1}^M$ such that $(\mathcal F, \Sigma, \mathcal V)$ covers $\mathcal C$
	\STATE
	\STATE $\mathcal Q \leftarrow \{c \in C_\cap \mid \Vert\textsc{Parents}(c)\Vert = 1 \}$
	\STATE $i \leftarrow 1$
	\STATE
	\WHILE{$\mathcal Q \neq \varnothing$}
	\STATE $c \leftarrow \textsc{Dequeue}(\mathcal Q)$
	\STATE $f_i(x) = \begin{cases}
	+ & \mbox{if } x \in c \\
	0 & \mbox{otherwise}
	\end{cases} $
	\STATE $\mathcal F \leftarrow \mathcal F \cup f_i$
	\STATE $i \leftarrow i + 1$
	\ENDWHILE
\end{algorithmic}

\section{Contrastive underspecification}
achieved by assigning a new feature [+f] to every segment in $X$, and if $Y \setminus X$ (the complement of $X$ with respect to $Y$) is in the input, then [-f] is assigned to every segment in $Y \setminus X$

\vspace{\baselineskip} \noindent \begin{algorithmic}
	\REQUIRE $\mathcal V = \{ +, -, 0 \}$
	\REQUIRE Precompute $C_\cap \leftarrow \textsc{IntersectionalClosure}(C)$
	\ENSURE Output featurization $\mathcal F = \{ f_j \}_{j=1}^M$ such that $(\mathcal F, \Sigma, \mathcal V)$ covers $\mathcal C$
	\STATE
	\STATE $\mathcal Q \leftarrow \{c \in C_\cap \mid \Vert\textsc{Parents}(c)\Vert = 1 \}$
	\STATE $i \leftarrow 1$
	\STATE
	\WHILE{$\mathcal Q \neq \varnothing$}
	\STATE $c \leftarrow \textsc{Dequeue}(\mathcal Q)$
	\STATE $p \leftarrow \textsc{Parents}(c)$
	\STATE $c' \leftarrow p \setminus c$
	\IF{$c' \in C$}
	\STATE $f_i(x) = \begin{cases}
	+ & \mbox{if } x \in c \\
	- & \mbox{if } x \in c' \\
	0 & \mbox{otherwise}
	\end{cases} $
	\STATE $\mathcal Q \leftarrow \{x \in \mathcal Q \mid x \not= c' \}$
	\ELSE
	\STATE $f_i(x) = \begin{cases}
	+ & \mbox{if } x \in c \\
	0 & \mbox{otherwise}
	\end{cases} $
	\ENDIF
	\STATE $\mathcal F \leftarrow \mathcal F \cup f_i$
	
	\STATE $i \leftarrow i + 1$
	\ENDWHILE
\end{algorithmic}

\section{Contrastive specification}
achieved by assigning a new feature [+f] to every segment in $X$, and [-f] to every segment in $Y \setminus X$ (even if $Y \setminus X$ was not in the input)

\vspace{\baselineskip} \noindent \begin{algorithmic}
	\REQUIRE $\mathcal V = \{ +, -, 0 \}$
	\REQUIRE Precompute $C_\cap \leftarrow \textsc{IntersectionalClosure}(C)$
	\ENSURE Output featurization $\mathcal F = \{ f_j \}_{j=1}^M$ such that $(\mathcal F, \Sigma, \mathcal V)$ covers $\mathcal C$
	
	\STATE
	\STATE $\mathcal Q \leftarrow \{c \in C_\cap \mid \Vert\textsc{parents}(c)\Vert = 1 \}$
	\STATE $i \leftarrow 1$
	\STATE
	\WHILE{$\mathcal Q \neq \varnothing$}
	\STATE $c \leftarrow \textsc{Dequeue}(\mathcal Q)$
	\STATE $p \leftarrow \textsc{Parents}(c)$
	\STATE $c' \leftarrow p \setminus c$
	\STATE $f_i(x) = \begin{cases}
	+ & \mbox{if } x \in c \\
	- & \mbox{if } x \in c' \\
	0 & \mbox{otherwise}
	\end{cases} $
	\STATE $C_\cap \leftarrow \textsc{IntersectionalClosure}(C_\cap \cup \{c'\})$
	\STATE $\mathcal Q \leftarrow \{c_l \in \mathcal Q \mid c_l \not= c' \ \land \Vert \textsc{Parents}(c_l) \Vert = 1 \}$
	\STATE $\mathcal F \leftarrow \mathcal F \cup f_i$
	
	\STATE $i \leftarrow i + 1$
	\ENDWHILE
\end{algorithmic}

\section{Full specification}
achieved by assigning a new feature [+f] to every segment in $X$, and [-f] to every segment in $\Sigma \setminus X$

\vspace{\baselineskip} \noindent \begin{algorithmic}
	\REQUIRE $\mathcal V = \{ +, -, 0 \}$
	\REQUIRE Precompute $C_\cap \leftarrow \textsc{IntersectionalClosure}(C)$
	\ENSURE Output featurization $\mathcal F = \{ f_j \}_{j=1}^M$ such that $(\mathcal F, \Sigma, \mathcal V)$ covers $\mathcal C$
	
	\STATE
	\STATE $\mathcal Q \leftarrow \{c \in C_\cap \mid \Vert\textsc{Parents}(c)\Vert = 1 \}$
	\STATE $i \leftarrow 1$
	\STATE
	\WHILE{$\mathcal Q \neq \varnothing$}
	\STATE $c \leftarrow \textsc{Dequeue}(\mathcal Q)$
	\STATE $c' \leftarrow \Sigma \setminus c$
	\STATE $f_i(x) = \begin{cases}
	+ & \mbox{if } x \in c \\
	- & \mbox{if } x \in c' \\
	0 & \mbox{otherwise}
	\end{cases} $
	\STATE $C_\cap \leftarrow \textsc{IntersectionalClosure}(C_\cap \cup \{c'\})$
	\STATE $\mathcal Q \leftarrow \{c_l \in \mathcal Q \mid c_l \not= c' \ \land \Vert \textsc{Parents}(c_l) \Vert = 1 \}$
	\STATE $\mathcal F \leftarrow \mathcal F \cup f_i$
	
	\STATE $i \leftarrow i + 1$
	\ENDWHILE
\end{algorithmic}

\appendix

\subsection{Proof of Lemma 1}

The proof proceeds by showing that $\langle \mathbf{e}_i \cup \mathbf{e}_j \rangle \subset  \langle \mathbf{e}_i \rangle \cap \langle \mathbf{e}_j \rangle$ and $ \langle \mathbf{e}_i \rangle \cap \langle \mathbf{e}_j \rangle \subset \langle \mathbf{e}_i \cup \mathbf{e}_j \rangle$.
Let $C_i = \langle \mathbf{e}_i \rangle$ and $C_j = \langle \mathbf{e}_j \rangle$.
First, suppose $x \in C_i \cap C_j$. Then $x \in C_i$. By definition, $x$ must have the features in $\mathbf{e}_i$.
Similarly, $x \in C_j$, and therefore must have the features in $\mathbf{e}_j$.
Thus, $x$ has the features in $\mathbf{e}_i \cup \mathbf{e}_j$. This shows that $C_i \cap C_j \subset \langle \mathbf{e}_i \cup \mathbf{e}_j \rangle$.
Now, suppose $x \in \langle \mathbf{e}_i \cup \mathbf{e}_j \rangle$. Then $x$ has all the features of $\mathbf{e}_i$, and so $x \in C_i$.
Similarly, $x$ has all the features of $\mathbf{e}_j$, so $x \in C_j$. Therefore $x \in C_i \cap C_j$. This shows that $\langle \mathbf{e}_i \cup \mathbf{e}_j \rangle \subset C_i \cap C_j$.
Since both $C_i \cap C_j$ and $\langle \mathbf{e}_i \cup \mathbf{e}_j \rangle$ are subsets of each other, they are equal.

\subsection{Proof of intersectional closure algorithm}

We will show that the algorithm generates every class in the intersectional closure by induction. $\mathcal C_\cap$ is initialized to contain $\Sigma$. Moreover, $\mathcal Q$ is initialized to contain every class in $\mathcal C$. Each of these must be `transferred' to the intersectional closure because they do not belong to it already (dequeued from $\mathcal Q$, and appended to $\mathcal C_\cap$). This demonstrates that every intersection of 0 classes ($\Sigma$) and 1 class from $\mathcal C$ (namely, $\mathcal C$ itself) belongs to $\mathcal C_\cap$. Now, suppose that the algorithm has guaranteed that every intersection of $n$ classes from $\mathcal C$ is in $\mathcal C_\cap$. If there exists a $Y \in \mathcal C_\cap$ which can be written as the intersection of $n+1$ classes, i.e. $Y = X_1 \cap X_2 \cap \ldots \cap X_{n+1} = Y' \cap X_{n+1}$ where $Y' = X_1 \cap X_2 \cap \ldots \cap X_n$. Since every intersection of $n$ classes is in $\mathcal C_\cap$, $Y'$ must be in $\mathcal C_\cap$. Now, regardless of whether $X_{n+1}$ was transferred from $\mathcal Q$ to $\mathcal C_\cap$ before or after $Y'$ was, there was some point at which one was in $\mathcal Q$ and the other in $\mathcal C_\cap$. When the \textbf{for} loop dequeued the one in $\mathcal Q$, it added the intersection of this one with all others in $\mathcal C_\cap$ -- i.e. $Y' \cap X_{n+1}$. Either this class was already in $\mathcal C_\cap$, or else it was not; and in the latter case, it was transferred. Thus, all sets generated by the intersection of $n+1$ classes from $\mathcal C$ are in $\mathcal C_\cap$. This completes the proof.



\subsubsection{Representing subset relations as matrices}

\vspace{\baselineskip} Any directed graph like Fig~\ref{fig:lattice} has an equivalent representation with a square matrix. Row $i$ is identified with class $C_i$, column $j$ with $C_j$; and the value in the $(i,j)$ cell indicates the relationship between $C_i$ and $C_j$. For binary relations like the subsethood and daughterhood, it is convenient to use the Boolean field: the set $\{T, \, F\}$ with operations $\cdot$ (logical \textsc{and}), $+$ (logical \textsc{or}), and additive inverse $-$ (logical \textsc{not}). The \textit{subset matrix} is defined thus:

$$S_{ij} = \begin{cases}
                T & \quad \text{if } C_j \subset C_i \\
                F & \quad \text{otherwise}
                \end{cases}$$

\noindent The advantage of writing a relation in this way is that fundamental matrix operations correspond to various measures of interest. For example, when $S$ is defined as above, and $S^2$ is calculated from ordinary matrix multiplication, then $S^2$ indicates the subset-of-a-subset relation, which corresponds graphically to `paths' of length 2 on a subset graph. In other words, $(S^2)_{ij} = T$ means there is a $C_k$ such that $S_{ik} = T \textsc{ and } S_{kj} = T$. If we further let \textsc{and} denote the element-wise multiplication operator, then the \textit{daughter matrix} can be computed from the subset matrix as follows: 

$$D = S \textsc{ and } -S^2$$

\noindent This matrix expresses the daughterhood relation: $D_{ij} = T$ means $C_j$ is a daughter of $C_i$.\footnote{Similarly, the subset matrix can be expressed as the sum of all powers of the daughter matrix, \mbox{$S = \sum_{n=1}^N D^n$}. For this problem, we generally expect to compute $D$ from $S$, rather than \textit{vice versa}.} The daughterhood relation is of central importance to the featurization algorithms described in this paper.

\bibliography{mybib.bib}
\bibliographystyle{apacite}

\end{document}