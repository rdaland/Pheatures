\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}        % so we can use the 'pretty' empty set
\usepackage{tipa}
\usepackage{graphicx}           % purdy pitchers
\usepackage{algorithmic}
\usepackage{phonrule}

\usepackage{cite}
\usepackage{apacite}

\newtheorem{definition}{Definition}

\title{An algorithm to assign features to a set of phonological classes}
\author{}
\author{
  Mayer, Connor \\
  \texttt{connormayer@ucla.edu}
  \and
  Daland, Robert \\
  \texttt{r.daland@gmail.com}
}
\date{\vspace{-5ex}}							% Activate to display a given date or no date

\begin{document}
\maketitle

\begin{abstract}
This squib describes a dynamic programming algorithm which assigns features to a set of phonological classes. The input consists of a set of classes, each containing one or more segments; in other words, a subset of the powerset of a segmental alphabet $\Sigma$. If a class can be generated as the union of existing features ( = intersection of already-processed classes), those features are propagated to every segment in the class. Otherwise, a new feature/value pair is assigned. The algorithm comes in 4 flavors, which differ with respect to complementation and how negative values are assigned. We show that these variants yield \textit{privative specification}, \textit{contrastive underspecification}, \textit{contrastive specification}, and \textit{full specification}, respectively. The main text sets out necessary background, and illustrates each variant of the algorithm. The Appendix formally proves that each algorithm is sound.
\end{abstract}

\section{Introduction}
% R sez: I'm not sure we want to use the word `distinctive' here.
%	My understanding is that the classical meaning of `distinctive' is that the presence/absence of the feature distinguishes two phonemes. For example, /p/ and /b/ are distinguished by the feature [voice]. I am not fully clear on how Jakobson and Trubetzkoy thought of things, but I believe that Jakobson was a chief proponent of binary specifications. For instance, he originated the concept of markedness, and his point was that it was more or less a purely formal thing -- one was marked and the other unmarked.
% There are other uses of features besides being distinctive. For example, some theorists believe that distinctive features are represented to the phonological grammar, while some additional (categorical) `phonetic' features are available to the phonetic grammar. One could imagine that the phonological grammar has sonorants specified as 0voice, while the phonetic grammar knows that they are phonetically voiced. One could similarly imagine there is a phonological +retroflex feature for Hindi (and non-retroflex sounds are phonetically interpreted as dental), while there is no such feature for English (and so the non-palatal coronals are phonetically interpreted as alveolar).
% I guess what I am saying is, in our case features do the work of distinguishing classes, but they don't necessarily distinguish phonemes. In fact, we don't actually mention phoneme vs. allophone or otherwise appeal to levels of representation in the rest of the paper -- so why would we invoke them in the first word?
% I went ahead and revised the first sentence since I feel somewhat strongly about it. But I otherwise try to keep most of your wording and such.

Features are the substantive building blocks of phonological theory. Features represent phonetic qualities of speech sounds, and can be used in isolation or combination to describe individual sounds or classes of sounds \cite<e.g.,>{Saussure1959, JakobsonEtAl1952}.

The goal of feature theory is to capture the generalization that segments which are made alike tend to behave alike. For example, the English voiceless stops [p, t, \textipa{\t{tS}}, k] are all produced with a complete, long-lag closure of the oral cavity, and exactly these segments undergo the process of foot-initial aspiration. The feature notation \phonfeat{-continuant \\ -voiced} exposes these shared phonetic properties to the phonological grammar and the processes which might reference them. One underpinning of feature theory is typological: segments which are made alike (in different languages) tend to behave alike (in different languages). For example, the feature \phonfeat{voice} is posited to explain the fact that in many languages, all obstruents undergo the same voicing processes (regressive voicing assimilation within obstruent clusters; word-final devoicing; intervocalic and/or postnasal voicing, etc...). % The example you had before did not explicitly indicate that these segments pattern similarly. That is why I added the foot-initial aspiration process. This still does not indicate patterning similarly across languages. That is why I also added voicing assimilation, which is relatively similar across lots of languages.

For this reason, classic texts \cite<e.g.,>{ChomskyHalle1968} have assumed phonological features are \textit{universal}: all the sounds in the world's languages can be described by the same finite set of features. Speakers inherently produce and perceive speech in terms of these features -- they are the substantive `atoms' of which segments and higher prosodic constituents are composed. These texts assume that children represent speech in terms of these atoms, which is why phonological processes operate on the classes they define.

Feature theory is manifestly successful in explaining why many common phonological processes involve segments that share relevant phonetic properties. However, it is also clear that many phonological processes target sets of segments that cannot be described by a set of shared features. One well-trodden example is the \textit{ruki} rule of Sanskrit, in which an underlying /s/ becomes retroflexed when it occurs after any of \{r, u, k, i\} \cite{TODO}. While it has been proposed that the \textit{ruki} process originated from the distinctive (though not uniform) effects of these segments on the second and third formant of neighboring segments \cite{TODO}, it is widely agreed that no conventional feature system can pick out all 4 of these segments to the exclusion of others \cite{TODO}. The issue is not limited to this one example. \citeA{Mielke2008} conducted a survey of phonological processes in almost 600 languages. In the `best' feature system he considered, 71\% of the classes which underwent or conditioned a phonological process could be expressed as the combination of simple features. The remaining 29\% are \textit{phonetically disparate classes} like \{r, u, k, i\}. Such classes require additional theoretical mechanisms -- such as building classes through an \textsc{OR} operation -- which seriously compromise the explanatory power that made feature theory appealing in the first place. 

The ubiquity of phonetically disparate classes has led some researchers to propose that distinctive features are \textit{learned} and \textit{language-specific} \cite<e.g.,>{Blevins2004, Mielke2008, MacWhinneyOGrady2015, ArchangeliPulleyblank2015}: learners are able to group sounds in their languages into classes regardless of whether they have any phonetic commonality. The striking regularities that exist across languages are explained as by-products of general human cognitive capabilities, such as categorization, sensitivity to frequency, and the ability to generalize, as well as the properties of the human vocal tract and auditory system.
% love this paragraph

This sets the stage for the goals of the current paper, which are somewhat modest. The basic question we address is the inverse of how features have typically been approached: rather than asking what classes a feature system defines, we instead focus on how a feature system can be learned from a set of predetermined classes. We begin by defining a formal notation for feature systems. We then describe the \textit{intersectional closure} of a set of classes, which must be generated by any featurization of that set. Using the intersectional closure as a tool for efficient calculation, we then describe a suite of algorithms for learning various types of featurizations for a set of input classes and prove their correctness, examining as we do the trade offs between number of classes and number of features that each featurization method makes.
% love this paragraph

This paper makes several important contributions: first, it demonstrates a method for working backwards to feature systems underpinning learned classes of sounds and provides the code for use in future research. Second, it provides a detailed formalization of what a featurization of classes entails, allowing careful reasoning about the expressiveness of such featurizations. Finally, by comparing multiple types of featurization of a set of classes, it makes explicit predictions about what classes should be describable under each type, which may be useful for future experimental phonological research.
% exceedingly, excessively love this paragraph
% you are a really clear writer when it comes to framing the paper in terms of larger goals

\section{Definitions and notation}

Let $\Sigma$ denote an alphabet of segments. We will use the term \textit{class} to mean a subset of $\Sigma$.

\subsection{Classes and class systems}

A \textit{class system} $(\mathcal C, \Sigma)$ consists of an alphabet $\Sigma$ and a set of classes $\mathcal C$ over that alphabet. Fig~\ref{fig:lattice} illustrates this definition with a natural class system over a set of vowels.

% show an example of a natural class system: a vowel harmony lattice
\begin{figure}[h]
\includegraphics[width=0.9\textwidth]{vowelHarmony_unicode.png}
\caption{An example vowel system, used throughout this paper}
\label{fig:lattice}
\end{figure}

In this graph, each node corresponds to a class, and a downward arrow from class $X$ to class $Y$ indicates that $Y \subset X$. However, the figure does not include an arrow for every pair of classes that have a subset relationship. For example, the class \{\textipa{E}\} is a subset of all vowels, but there is not an arrow from the class of all vowels to \{\textipa{E}\}. This is because the class of front vowels \{\textipa{\oe}, \textipa{y}, \textipa{E}, \textipa{i}\} `intervenes' between \{\textipa{E}\} and the class of all vowels. We will use the terms \textit{parent/daughter} to refer to cases in which a subset/superset relation holds and there is no intervening class.

Graphically, it is convenient to indicate only parent-daughter relations in figures, since other superset-subset relations are entailed. However, the parenthood relation is not only useful for graphing. As we show later, the parenthood relation is essential for the featurization algorithm. Thus, we formalize the definition here: \begin{itemize}
    \item $X$ is a \textit{parent} of $Y$ (with respect to $\mathcal C$) if and only if $Y \subset X$, and $\nexists W \in \mathcal C \, [Y \subset W \subset X]$
    \end{itemize}

\noindent We further define $\textsc{parents}(Y, \mathcal C)$ as the set of all parents of $Y$ (with respect to $\mathcal C$). 

\subsection{Feature systems and featurizations}

A \textit{feature system} is a tuple $(\mathcal F, \Sigma, \mathcal V)$ where \begin{itemize}
    \item $\Sigma$ is a segmental alphabet, 
    \item $\mathcal V$ is a set of values, and 
    \item $\mathcal F$ is a \textit{featurization}: a set of features $\{f_j\}_{j=1}^M$, where each feature is a function $f: \Sigma \rightarrow \mathcal V$ mapping segments to feature values
    \end{itemize}

\noindent To illustrate, a feature system for the vowel system of Fig.~\ref{fig:lattice} is shown below in Table~\ref{table:featurization} (the empty set is omitted in this and all similar figures). In the next subsection we formalize featural descriptors, which relate classes and feature systems.

\begin{table}[h]
    \centering
    \begin{tabular} {|c||c|c|c|c|c|}
    \hline
        $\sigma$ & front & back & low & high & round \\ \hline
        \textipa{i} & + & -- & -- & + & -- \\
        \textipa{y} & + & -- & -- & + & + \\
        \textipa{W} & -- & + & -- & + & -- \\
        \textipa{u} & -- & + & -- & + & + \\
        \textipa{E} & + & -- & -- & -- & -- \\
        \textipa{\oe} & + & -- & -- & -- & + \\
        \textipa{2} & -- & + & -- & -- & -- \\
        \textipa{O} & -- & + & -- & -- & + \\
        \textipa{a} & -- & + & + & -- & -- \\
        \hline
    \end{tabular}
    \caption{Example of a feature system.}
    \label{table:featurization}
\end{table}

\subsection{Featural descriptors}

Let $(\mathcal F, \Sigma, \mathcal V)$ be a feature system. We restrict $\mathcal V$ to the following possibilities: \begin{itemize}
    \item \textit{privative specification}: $\mathcal V = \{ +, 0 \}$
    \item \textit{full specification}: $\mathcal V = \{ +, - \}$
    \item \textit{contrastive specification}: $\mathcal V = \{ +, -, 0 \}$
    \end{itemize}

A \textit{featural descriptor} $\mathbf{e}$ is a set of feature/value pairs where the values cannot be $0$, i.e. $\mathbf{e} \subset (\mathcal V \setminus \{0\}) \times \mathcal F$. For example, $\mathbf{e} =$  \phonfeat{+front \\ -low} is a featural descriptor.

To relate featural descriptors and phonological classes, note that every featural descriptor $\mathbf{e}$ can be expressed in the form $\mathbf{e} = \{\alpha_k F_k\}_{k=1}^K$, where each $\alpha_k$ is a value in $\mathcal V \setminus \{ 0 \}$, and each $F_k$ is some feature function $f_j \in \mathcal F$. Informally, we say that a featural descriptor describes the class of segments which have (at least) the feature/value pairs it contains. Formally, we write $\langle \mathbf{e} \rangle$ to indicate the natural class that corresponds to the featural descriptor $\mathbf{e}$:

$$ \langle \, \{\alpha_k F_k\}_{k=1}^K \, \rangle = \{x \in \Sigma \, \mid \, F_k(x) = \alpha_k \text{ for every } k \} $$

\vspace{\baselineskip} \noindent We use the notation $\mathcal V^\mathcal F$ to denote the powerset of $(\mathcal V \setminus \{0\}) \times \mathcal F$, i.e. the set of all licit featural descriptors. Lastly, we define $\langle \mathcal V^\mathcal F \rangle = \{ \langle \mathbf{e} \rangle \, \mid \, \mathbf{e} \in \mathcal V^\mathcal F \}$, the set of all natural classes described by some featural descriptor in $\mathcal V^\mathcal F$. We say that the feature system $(\mathcal F, \Sigma, \mathcal V)$ generates the natural class system $\langle \mathcal V^\mathcal F \rangle$.

Note that while every featural descriptor in $\mathcal V^\mathcal F$ picks out a class in $\langle \mathcal V^\mathcal F \rangle$, the two are not generally in 1-1 correspondence. This is because the same class can often be described by multiple featural descriptors. For example, under the the feature system of Table~\ref{table:featurization}, the featural descriptor \phonfeat{+front} picks out the same class as the featural descriptor \phonfeat{+front \\ -low} (namely, the front vowels). Moreover, the featural descriptors \phonfeat{+front \\ -front} and \phonfeat{+high \\ +low} both pick out the empty set.

\vspace{\baselineskip} We say that a feature system $(\mathcal F, \Sigma, \mathcal V)$ \textit{covers} a natural class system $\mathcal C$ if $\mathcal C \subset \langle \mathcal V^\mathcal F \rangle$; in other words if the feature system provides a distinct featural representation for every class in $\mathcal C$. In the remainder of this squib, we show how to construct a feature system that covers an arbitrary natural class system $\mathcal C$.

We begin with a worked-out example illustrating the difference between privative and full specification with the same segmental alphabet. Then we introduce the notion of intersectional closure, which leads naturally to a featurization algorithm for privative specification. Simple modifications yield algorithms for contrastive and full specification.

\subsection{Example}

Let $\Sigma =$ \{R, D, T\}. Informally, the reader may think of [R] as a sonorant, [D] as a voiced obstruent, and [T] as a voiceless obstruent; accordingly we use the feature names $son$ and $vcd$. In this section, we illustrate the consequences of privative versus full specification, using featurizations which match on the `$+$' values, and differ only as to whether the other values are `$0$' or `$-$'). We begin with Table ~\ref{table:privative}.

% table with featurization of sonorants, voiced obstruents, and voiceless obstruents
\begin{table}[h]
    \centering
    \begin{tabular} {|c||c|c|}
    \hline
        $\sigma$ & son & vcd \\ \hline
        R & + & + \\
        D & 0 & + \\
        T & 0 & 0 \\
        \hline
    \end{tabular}
    \caption{Sonorants and obstruents with privative specification.}
    \label{table:privative}
\end{table}

\noindent The set of natural classes it describes, and the simplest featural descriptor for each, are shown below: \begin{itemize}
  \item $[\,]$ -- \{R, D, T\}
  \item $[+\text{son}]$ -- \{R\}
  \item $[+\text{vcd}]$ -- \{R, D\}
  \end{itemize}
  
\noindent Note that this featurization provides no featural descriptor that uniquely picks out the voiceless obstruent [T], no way to pick out the obstruents [T] and [D] to the exclusion of [R], and no way to pick out the voiced obstruent [D] without [R].

Next, consider the featurization in which the `$0$'s from Table ~\ref{table:privative} are replaced with `$-$'s:

% table with featurization of sonorants, voiced obstruents, and voiceless obstruents
\begin{table}[h]
    \centering
    \begin{tabular} {|c||c|c|}
    \hline
        $\sigma$ & son & vcd \\ \hline
        R & + & + \\
        D & -- & + \\
        T & -- & -- \\
        \hline
    \end{tabular}
    \caption{Sonorants and obstruents with full specification.}
    \label{table:full}
\end{table}

\noindent The set of natural classes this featurization describes is much larger, because the number of (extensionally distinct) featural descriptors is larger: \begin{itemize}
    \item $[\,] =$ \{R, D, T\}
    \item $[+\text{son}] =$ \{R\}
    \item $[-\text{son}] =$ \{D, T\}
    \item $[+\text{vcd}] =$ \{R, D\}
    \item $[-\text{vcd}] =$ \{T\}
    \item $[-\text{son},+\text{vcd}] =$ \{D\}
    \item $[+\text{son},-\text{son}] = \varnothing$
    \end{itemize}

\noindent An important generalization emerges from comparing these featurizations: the more `$0$'s in the featurization, the greater the number of distinct feature functions that will be required to cover the same natural class system. In one sense, privative specification is more complex, because it will normally involve more features. However, in another sense, it is simpler, because there are only `$+$' values to handle and because it will result in fewer natural classes. Therefore, we will treat privative specification first. Prior to this, we introduce the notion of intersectional closure -- the data structure that proves useful for efficiently assigning a privative feature system.

\section{Intersectional closure}

In this section we define the \textit{intersectional closure} of a natural class system $\mathcal C$ as the set of classes that can be generated by intersecting $\Sigma$ with any set of classes in $\mathcal C$. We relate the intersectional closure to features by showing that if a feature system is expressive enough to generate all the classes in $\mathcal C$, it generates the intersectional closure. Then we give a dynamic programming algorithm which efficiently computes the intersectional closure. 

\subsection{Definition}

A collection of sets $\mathcal C$ is \textit{intersectionally closed} if and only if $\forall (X, Y \in \mathcal C) \, [ X \cap Y \in \mathcal C]$.

We write $\mathcal C_\cap$ to indicate the \textit{intersectional closure} of a natural class system $(\mathcal C, \Sigma)$. This the smallest intersectionally closed collection which contains $\Sigma$ and every set in $\mathcal C$; in other words, the intersectional closure does not contain any classes except $\Sigma$ and those which can be generated by finite intersections of classes from $\mathcal C$.

\subsection{Feature systems generate an intersectional closure}

Now we explain why any feature system that covers $\mathcal C$ must cover $\mathcal C_\cap$. The explanation rides on the dual relationship between featural descriptors and the classes they describe: the class described by the union of two featural descriptors is the intersection of the classes described by each of the descriptors alone. This principle can be illustrated with the following example, using the vowel system in Fig.~\ref{fig:lattice}. Let $\mathbf{e}_1 =$ \phonfeat{+front} and $\mathbf{e}_2 =$ \phonfeat{+round}. Then \begin{itemize}
    \item $\langle$\phonfeat{+front}$\rangle =$ \{\textipa{\oe}, \textipa{y}, \textipa{E}, \textipa{i}\}
    \item $\langle$\phonfeat{+round}$\rangle =$ \{\textipa{\oe}, \textipa{o}, \textipa{y}, \textipa{u}\}
    \item $\langle$\phonfeat{+front}$\rangle \cap \langle$\phonfeat{+round}$\rangle =$ \{\textipa{\oe}, \textipa{y}\}
    \item $\langle$\phonfeat{+front,+round}$\rangle =$ \{\textipa{\oe}, \textipa{y}\}
    \end{itemize}

In other words, the set of vowels that are both front and round is the intersection of the set of vowels that are front and the set of vowels that are round. The Featural Intersection Lemma proves that this kind of relationship holds for any pair of featural descriptors (and the classes they describe).

\vspace{\baselineskip} \noindent \textbf{Featural Intersection Lemma}

Let $(\mathcal F, \Sigma, \mathcal V)$ be a feature system. If $\mathbf{e}_i, \mathbf{e}_j \in \mathcal V^\mathcal F$, then $\langle \mathbf{e}_i \cup \mathbf{e}_j \rangle =  \langle \mathbf{e}_i \rangle \cap \langle \mathbf{e}_j \rangle$.

\vspace{\baselineskip} \noindent \textit{Proof}:

The proof proceeds by showing that $\langle \mathbf{e}_i \cup \mathbf{e}_j \rangle \subset  \langle \mathbf{e}_i \rangle \cap \langle \mathbf{e}_j \rangle$ and $ \langle \mathbf{e}_i \rangle \cap \langle \mathbf{e}_j \rangle \subset \langle \mathbf{e}_i \cup \mathbf{e}_j \rangle$.
Let $C_i = \langle \mathbf{e}_i \rangle$ and $C_j = \langle \mathbf{e}_j \rangle$.
First, suppose $x \in C_i \cap C_j$. Then $x \in C_i$. By definition, $x$ must have the features in $\mathbf{e}_i$.
Similarly, $x \in C_j$, and therefore must have the features in $\mathbf{e}_j$.
Thus, $x$ has the features in $\mathbf{e}_i \cup \mathbf{e}_j$. This shows that $C_i \cap C_j \subset \langle \mathbf{e}_i \cup \mathbf{e}_j \rangle$.
Now, suppose $x \in \langle \mathbf{e}_i \cup \mathbf{e}_j \rangle$. Then $x$ has all the features of $\mathbf{e}_i$, and so $x \in C_i$.
Similarly, $x$ has all the features of $\mathbf{e}_j$, so $x \in C_j$. Therefore $x \in C_i \cap C_j$. This shows that $\langle \mathbf{e}_i \cup \mathbf{e}_j \rangle \subset C_i \cap C_j$.
Since both $C_i \cap C_j$ and $\langle \mathbf{e}_i \cup \mathbf{e}_j \rangle$ are subsets of each other, they are equal.
This completes the proof.

\vspace{\baselineskip} \noindent The key utility of this Lemma is that it can be applied inductively, to relate the union of multiple featural descriptors with the intersection of multiple classes.

\vspace{\baselineskip} \noindent \textbf{Intersectional Closure Covering Theorem}

Let $(\mathcal C, \Sigma)$ be a natural class system and $(\mathcal F, \Sigma, \mathcal V)$ a feature set. If $\mathcal C \subset \langle \mathcal V^\mathcal F \rangle$, then $\mathcal C_\cap \subset\langle \mathcal V^\mathcal F \rangle $.

\vspace{\baselineskip} \noindent \textit{Proof}:

Let $Y$ be an arbitrary class in $\mathcal C_\cap$. By definition of $\mathcal C_\cap$, there exist $\{X_i \in \mathcal C\}_{i \in I}$ (for some index set $I$, hereafter omitted) such that and $Y = \bigcap_i \, X_i$. The hypothesis that $\mathcal C \subset \langle \mathcal V^\mathcal F \rangle $ implies that for every such $X_i$, there exists a featural descriptor $\mathbf{e}_i$ such that $\langle \mathbf{e}_i \rangle = X_i$. Thus, $Y = \bigcap_i X_i = X_1 \cap X_2 \cap \ldots \cap X_n$ can also be written $C = \bigcap_i \, \langle \mathbf{e}_i \rangle = \langle \mathbf{e}_1 \rangle \cap \langle \mathbf{e}_2 \rangle \cap \ldots \cap \langle \mathbf{e}_n \rangle$. It follows by induction using Featural Intersection Lemma that $Y = \langle \bigcup_i \mathbf{e}_i \rangle$:

$Y = \langle \mathbf{e}_1 \rangle \cap  \langle \mathbf{e}_2 \rangle \cap \ldots \cap  \langle \mathbf{e}_n \rangle$

\quad $ = \langle \mathbf{e}_1 \cup \mathbf{e}_2 \rangle \cap \mathbf{e}_3 \cap \ldots \cap \langle \mathbf{e}_n \rangle$

\quad $ = \langle \mathbf{e}_1 \cup \mathbf{e}_2 \cup \mathbf{e}_3 \rangle \cap \ldots \cap \langle \mathbf{e}_n \rangle$

\quad $\ldots$

\quad $= \langle \mathbf{e}_1 \cup \mathbf{e}_2 \cup \ldots \cup \mathbf{e}_n \rangle$

\quad $= \langle \bigcup_i  \mathbf{e}_i \rangle$

\noindent The preceding chain of logic demonstrates that if a class can be expressed as the intersection of natural classes in $\mathcal C$, then its features are the union of the features in each of those classes. The intersectional closure is defined as all possible intersections of classes in $\mathcal C$. Thus, if $(\mathcal F, \Sigma, \mathcal V)$ covers $\mathcal C$, it covers the intersectional closure. This completes the proof.

\subsection{An algorithm for calculating the intersectional closure}

The following algorithm yields the intersectional closure of a natural class system $(\mathcal C, \Sigma)$. A proof by induction is given in the Appendix.

\noindent \begin{algorithmic}
    \ENSURE $\mathcal C_\cap$ is the intersectional closure of the input $\mathcal C$
    \STATE
    \STATE $\mathcal C_\cap \leftarrow \{ \Sigma \} $
    \STATE $\mathcal Q \leftarrow \mathcal C$
    \STATE
    \WHILE{$\mathcal Q \neq \varnothing$}
        \STATE $X \leftarrow \textsc{dequeue}(\mathcal Q)$
        \IF{\NOT $X \in \mathcal C_\cap$}
            \FOR{$Y \in \mathcal C_\cap$}
                \STATE $\textsc{enqueue}(\mathcal Q, X \cap Y)$
            \ENDFOR
            \STATE $\textsc{append}(\mathcal C_\cap, \, X)$
        \ENDIF
    \ENDWHILE
\end{algorithmic}

\vspace{\baselineskip} \noindent \textit{Proof of soundness}:

The proof goes by induction. First, we show that every class which can be generated by the intersection of $0$ classes ($\Sigma$) or 1 class from $\mathcal C$ (i.e. $\mathcal C$ itself) belongs to $\mathcal C_\cap$. Next, we prove the induction step: if every class that can be generated by the intersection of $n$ classes from $\mathcal C$ is in $\mathcal C_\cap$, then every class that can be generated by the intersection of $n+1$ classes from $\mathcal C$ is in $\mathcal C_\cap$.

Observe that $\mathcal C_\cap$ is initialized to contain $\Sigma$. Moreover, $\mathcal Q$ is initialized to contain every class in $\mathcal C$. Each of these must be `transferred' to the intersectional closure because they do not belong to it already (dequeued from $\mathcal Q$, and appended to $\mathcal C_\cap$). This demonstrates that every intersection of 0 classes ($\Sigma$) and 1 class from $\mathcal C$ (namely, $\mathcal C$ itself) belongs to $\mathcal C_\cap$.

Now, suppose that the algorithm has guaranteed that every intersection of $n$ classes from $\mathcal C$ is in $\mathcal C_\cap$. If there exists a $Y \in \mathcal C_\cap$ which can be written as the intersection of $n+1$ classes, i.e. $Y = X_1 \cap X_2 \cap \ldots \cap X_{n+1} = Y' \cap X_{n+1}$ where $Y' = X_1 \cap X_2 \cap \ldots \cap X_n$. Since every intersection of $n$ classes is in $\mathcal C_\cap$, $Y'$ must be in $\mathcal C_\cap$. Now, regardless of whether $X_{n+1}$ was transferred from $\mathcal Q$ to $\mathcal C_\cap$ before or after $Y'$ was, there was some point at which one was in $\mathcal Q$ and the other in $\mathcal C_\cap$. When the \textbf{for} loop dequeued the one in $\mathcal Q$, it added the intersection of this one with all others in $\mathcal C_\cap$ -- i.e. $Y' \cap X_{n+1}$. Either this class was already in $\mathcal C_\cap$, or else it was not; and in the latter case, it was transferred. Thus, all sets generated by the intersection of $n+1$ classes from $\mathcal C$ are in $\mathcal C_\cap$. This completes the proof.


\subsection{Parenthood in the intersectional closure}

As we will see shortly, the advantage of explicitly computing the intersectional closure is that \textit{a new feature is required for all and only the classes which have a single parent} (in the intersectional closure). The core reason for this is that if a class has two parents, it must be their intersection. We prove this here.

\vspace{\baselineskip} \noindent \textbf{Single Parenthood Theorem}

Let $(\mathcal C, \Sigma)$ be a natural class system and $Y \in \mathcal C_\cap$. If $X_1, X_2 \in \textsc{parents}(Y)$, then $Y = X_1 \cap X_2$.

\vspace{\baselineskip} \noindent \textit{Proof}:

First, observe that $Y \subset X_1 \cap X_2$. This follows trivially from the definition of parenthood: $X_1$ is a parent of $Y$ implies $Y \subset X_1$, $X_2$ is a parent of $Y$ implies $Y \subset X_2$, and so every element in $Y$ is in both $X_1$ and $X_2$.

Now suppose that $X_1 \cap X_2 \neq Y$. The preceding logic showed that either the two are equal, or $Y$ is a proper subset of  $X_1 \cap X_2$. But the latter case creates a contradiction. By definition, $(X_1 \cap X_2)$ must be in the intersectional closure, and $X_1 \cap X_2 \subset X_1$ follows from fundamental properties of sets. Then $X_1 \cap X_2$ intervenes between $Y$ and $X_1$, contradicting the hypothesis that $Y$ is a daughter of $X_1$. Thus, $Y = X_1 \cap X_2$.
	
\vspace{\baselineskip} Note that the Single Parenthood Theorem does not logically exclude the possibility that a class may have more than two parents. Rather, it guarantees that in such cases, the intersection is the same regardless of how many parents are considered. One case in which this can happen is the null set: if $x, y, z$ are three distinct elements from $\Sigma$, then $\{ x \} \cap \{ y \} = \varnothing = \{ y \} \cap \{ z \}$. A more interesting case arises in the intersectional closure of the vowel system in Fig.~\ref{fig:lattice}, shown in Fig.~\ref{fig:closure}. 

\begin{figure}[h]
\includegraphics[width=0.9\textwidth]{vowelHarmony_closure.png}
\caption{Intersectional closure of the vowel system shown earlier}
\label{fig:closure}
\end{figure}

In this case, the largest daughters of $\Sigma$ are the \phonfeat{+front} vowels, the \phonfeat{+high} vowels, and the \phonfeat{+round} vowels. The pairwise intersections of these classes give rise to the \phonfeat{+front \\ +high}, \phonfeat{+high \\ +round}, and \phonfeat{+round \\ +front} classes. The intersection of any pair of these is \{\textipa{y}\} (the high, front, round vowel), and the intersection of all 3 is also \{\textipa{y}\}. Thus, this set has 3 parents, but the segments it contains are determined simply by having more than 1 parent. In the next section, we give an algorithm which generates a privative feature system that covers the intersectional closure $\mathcal C_\cap$, given the `input' of a class system $(\mathcal C, \Sigma)$.

 \section{Privative specification}

The following algorithm yields a privative specification by assigning a different feature \phonfeat{+f} to the segments in each class with a single parent.
 
\noindent \begin{algorithmic}
    \REQUIRE $\mathcal C_\cap$ is the intersectional closure of a natural class system $(\mathcal C, \Sigma)$
    \ENSURE $\mathcal F$ is a featurization over $\mathcal V = \{ +, 0 \}$ which covers $\mathcal C$
    \STATE
    \STATE $\mathcal Q \leftarrow \mathcal C_\cap$
    \STATE $\mathcal F \leftarrow \varnothing$
    \STATE
    \WHILE{$\mathcal Q \neq \varnothing$}
        \STATE $X \leftarrow \textsc{pop}(\mathcal Q)$
        \IF{$|\textsc{parents}(X)| = 1$}
            \STATE define $f_X : \Sigma \rightarrow \mathcal V$ by $f_X(\sigma) = \begin{cases}
                + & \mbox{if } \sigma \in X \\
                0 & \mbox{otherwise}
                \end{cases} $
            \STATE $\textsc{append}(\mathcal F, f_X)$
        \ENDIF
    \ENDWHILE
\end{algorithmic}

\vspace{\baselineskip} \noindent \textit{Proof of soundness for the privative specification algorithm}

A featurization algorithm is \textit{sound} if for every class system $(\mathcal C, \Sigma)$, it returns a feature system which covers $\mathcal C$. To see that the privative specification algorithm is sound, note that every class in $\mathcal C_\cap$ enters the queue $\mathcal Q$. For an arbitrary class $X$ in the queue, there are 3 cases. If $X$ has 0 parents, then it is $\Sigma$, and is covered by the empty featural descriptor. If $X$ has exactly 1 parent, then the segments in $X$ get the features of that parent (which uniquely pick out the parent class), plus a new feature $f$ which distinguishes the segments in $X$ from $X$'s parent. If $X$ has more than 1 parent, then Single Parenthood Theorem shows, via the Featural Intersection Lemma, that the union of features of $X$'s parents uniquely pick out all and only the segments in $X$. Thus, each class which exits the queue has a set of features assigned to its segments which pick out that class uniquely. This completes the proof.

\vspace{\baselineskip} In Fig.~\ref{fig:privative}, we illustrate the outcome of applying the privative specification algorithm to the intersectional closure of Fig.~\ref{fig:closure}. We employ several conventions to jointly optimize readability and informativity. First, classes which have a single parent are visually highlighted using a thick, red ellipse. These represent the classes that cannot be featurized simply by the union of their parents' features. Second, the arrow which leads to each such class is annotated with the feature/value that is used to distinguish it. This represents the `point' at which each feature is assigned. Note that since the algorithm assigns feature/value pairs to \textit{segments}, every daughter of the class which got a new feature also gets that feature. The set of features that is shared by all members of a class, i.e. which uniquely picks out that class and not any other, is represented under the node. The complete featurization assigned to each segment is thus represented by inspecting the singleton sets in the bottom row. For readability, we use feature names that are familiar from phonological theory when the feature picks out more than segment in isolation (but note that the algorithm knows nothing of phonetic substance -- as far as the algorithm is concerned, they are just arbitrary symbols). When the feature only picks out one segment, we name the feature after the segment.

\begin{figure}[h]
\includegraphics[width=0.9\textwidth]{vowelHarmony_privative.png}
\caption{Yield of the privative specification algorithm}
\label{fig:privative}
\end{figure}

We close this section with some observations on the properties of the privative specification algorithm and the featurization it yields. 

\subsection{Properties of privative specification}

One point to observe is that the privative specification algorithm is \textit{maximally conservative}. What we mean by this is that the resulting feature system generates the smallest natural class system that covers $\mathcal C$. As the Intersectional Closure Covering Theorem showed, any featurization which covers $\mathcal C$ will cover $\mathcal C_\cap$. This means that any classes which are the intersection of input classes, but which were not themselves in the input, will be `accessible' to the output feature system. But the privative specification algorithm will not make it possible to refer to any other classes, besides those necessary ones. For example, if the input contains a \phonfeat{+front} class and a \phonfeat{+round} class, it must generate a \phonfeat{+front \\ +round} class, but it will not `create' a \phonfeat{-round} class.

This might be the desired behavior. But other properties might be desired instead. For instance, one might have theoretical grounds for wishing to allow `$-$' values. One might also wish to have an \textit{economical} feature system -- one which minimizes the number of features needed to cover $\mathcal C$. It is easy to show that one can sometimes achieve a more economical  feature system by `adding' classes to the system. For example, the featurization shown in Fig.~\ref{fig:privative} contains 10 features (\textit{front}, \textit{high}, \textit{round}, plus 7 features for the individual segments that cannot be accessed as combinations of front, high, and round). It is left as an exercise for the reader to verify that if the input consists of the following classes, the privative specification algorithm returns a featurization with 7 features: \begin{itemize}
    \item \textit{front} -- \{\textipa{i},\textipa{y}, \textipa{E}, \textipa{\oe}\}
    \item \textit{back} -- \{\textipa{W}, \textipa{u}, \textipa{2}, \textipa{O}\}
    \item \textit{round} -- \{\textipa{y}, \textipa{u}, \textipa{\oe}, \textipa{O}\}
    \item \textit{unround} --  \{\textipa{i}, \textipa{W}, \textipa{E}, \textipa{2}, \textipa{a}\}
    \item \textit{high} --  \{\textipa{i}, \textipa{y}, \textipa{W}, \textipa{u}\}
    \item \textit{low} -- \{\textipa{a}\}
    \item \textit{mid} --  \{\textipa{E}, \textipa{\oe}, \textipa{2}, \textipa{O}\}
    \end{itemize}
Crucially, this featurization covers the original class system shown in Fig.~\ref{fig:lattice}. In other words, it uses less features while generating a richer class system.

This example is presented to make two points. First, the relationship between classes in the input and the specification algorithm is not monotone. In general, adding features to a system will make more classes accessible -- but in this example, a smaller number of features covers a larger class system. Thus, the minimal number of features needed to cover $\mathcal C$ is not predictable from a `simple' property, such as the total number of classes in $\mathcal C$. To be more precise, the proof of soundness of the privative specification algorithm gives an upper bound on the features needed to cover a class system (namely, the number of classes in the intersectional closure with a single parent). We return to the issue of feature economy and expressiveness in the Discussion section. In the meantime, we turn to the second point this example makes -- adding the `right' classes to the input is what enabled a more economical feature system. In the next sections, we explore variants of the privative specification algorithm which consider complement classes and assign `$-$' values instead of (or in addition to) `$0$' values. 

\section{Contrastive underspecification}

One of the best cases for non-privative specifications arise from complement classes, such as round vs. nonround vowels, or voiced vs. voiceless obstruents. In a language with rounding harmony, like Turkish, one would need to write one harmony rule for the \phonfeat{+round} feature, and an otherwise identical rule for the \phonfeat{+nonround} feature. By allowing features to take on opposing values, one formally recognizes the sameness of rounding with respect to the harmony process.

In canonical cases like rounding harmony and voicing assimilation, the binary feature is only relevant for certain segments. For example, in the case of rounding harmony, it is normally useful to assign the \phonfeat{+round} and \phonfeat{-round} values only to vowels. In some languages, one might wish to only assign these values to just non-low vowels, or just front vowels. In all such cases, the contrasting feature values denote complementary classes -- but complements with respect to \textit{what}?

The central insight developed in this paper is that a new feature needs to be assigned just in case a class has a single parent. This suggests that the relevant domain for complementation is with respect to the parent. And that is the distinction between privative specification and contrastive underspecification: a `$-$' value is assigned when the complement of the class being processed (with respect to its parent) is in the input.

\vspace{\baselineskip} \noindent \begin{algorithmic}
    \REQUIRE $\mathcal C_\cap$ is the intersectional closure of input class system $(\mathcal C, \Sigma)$
    \ENSURE $\mathcal F$ is a featurization over $\mathcal V = \{ +, -, 0 \}$ which covers $\mathcal C$
    \STATE
    \STATE $\mathcal Q \leftarrow \mathcal C_\cap$
    \STATE $\mathcal F \leftarrow \varnothing$
    \STATE
    \WHILE{$\mathcal Q \neq \varnothing$}
        \STATE $X \leftarrow \textsc{dequeue}(\mathcal Q)$
        \IF{$| \textsc{parents}(X) | = 1$}
            \STATE $P_X \leftarrow \textsc{dequeue}(\textsc{parents}(X))$
            \STATE
            \STATE $\overline{X} \leftarrow \begin{cases}
                P_X \setminus X & \text{if } (P_X \setminus X) \in \mathcal C \\
                \varnothing     & \text{otherwise}
                \end{cases}$
            \STATE
            \STATE define $f_X : \Sigma \rightarrow \mathcal V$ by $f_X (\sigma) = \begin{cases}
                    + & \text{if } \sigma \in X \\
                    - & \text{if } \sigma \in \overline{X} \\
                    0 & \text{otherwise}
                    \end{cases}$
            \STATE $\textsc{append}(\mathcal F, f_X)$
        \ENDIF
    \ENDWHILE
\end{algorithmic}

\vspace{\baselineskip} \noindent The soundness of this algorithm follows from the soundness of the privative specification algorithm. That is because the contrastive underspecification algorithm yields a feature system which generates the same class system as privative specification does. The only difference between the two is that if the input contains complement sets, then contrastive underspecification will use a single feature with `$+$' and `$-$' values, where privative specification will have two features with just `$+$' values.

\vspace{\baselineskip} We illustrate this algorithm on the 3-segment system \{R, T, D\} discussed before. Suppose that the input consists of the following: \begin{itemize}
    \item \textit{obstruents} -- \{D, T\}
    \item \textit{sonorants} -- \{R\}
    \item \textit{voiced obstruents} -- \{D\}
    \item \textit{voiceless obstruents} -- \{T\}
    \end{itemize}

\noindent Then contrastive underspecification will yield the following featurization (or one which is equivalent to it, but with inverse signs):

% table with featurization of sonorants, voiced obstruents, and voiceless obstruents
\begin{table}[h]
    \centering
    \begin{tabular} {|c||c|c|}
    \hline
        $\sigma$ & obstr & vcd \\ \hline
        R & -- & 0 \\
        D & + & + \\
        T & + & -- \\
        \hline
    \end{tabular}
    \caption{Sonorants and obstruents with contrastive underspecification.}
    \label{table:underspecification}
\end{table}

The term contrastive underspecification is meant to capture that features can be binary or privative; segments will be underspecified with respect to a feature if the relevant complement class is not included in the input. For example, in Table~\ref{table:underspecification} the segment $R$ is not specified for voicing -- but it would have been if the input had included the class \{R, D\}. In the next section, we consider a variant of the algorithm which adds the complement class, even if it wasn't present in the input. We call this variant contrastive specification.
    
\section{Contrastive specification}

Contrastive specification is very similar to contrastive underspecification. The key difference is that contrastive specification adds classes to the covering. Every complement gets a `$-$' feature, including those which were not in the input. Because of this, the intersectional closure actually changes throughout the computation. It can be updated dynamically, by running the intersectional closure algorithm, except starting with the pre-existing closure, and a queue consisting of the novel complement class.

\vspace{\baselineskip} \noindent \begin{algorithmic}
    \REQUIRE $\mathcal C_\cap$ is the intersectional closure of input class system $(\mathcal C, \Sigma)$
    \ENSURE $\mathcal F$ is a featurization over $\mathcal V = \{ +, -, 0 \}$ which covers $\mathcal C$
    \STATE
    \STATE $\mathcal Q \leftarrow \mathcal C_\cap$
    \STATE $\mathcal F \leftarrow \varnothing$
    \STATE
    \WHILE{$\mathcal Q \neq \varnothing$}
        \STATE $X \leftarrow \textsc{dequeue}(\mathcal Q)$
        \IF{$| \textsc{parents}(X) | = 1$}
            \STATE $P_X \leftarrow \textsc{dequeue}(\textsc{parents}(X))$
            \STATE $\overline{X} \leftarrow P_X \setminus X$
            \STATE define $f_X : \Sigma \rightarrow \mathcal V$ by $f_X (\sigma) = \begin{cases}
                    + & \text{if } \sigma \in X \\
                    - & \text{if } \sigma \in \overline{X} \\
                    0 & \text{otherwise}
                    \end{cases}$
            \STATE $\textsc{append}(\mathcal F, f_X)$
            \STATE
            \STATE $\mathcal C_\cap \leftarrow \textsc{IntersectionalClosure}(\mathcal C_\cap, \mathcal Q' = \{\overline{X}\})$
        \ENDIF
    \ENDWHILE
\end{algorithmic}

\vspace{\baselineskip} \noindent This algorithm is sound because it considers all the classes that the privative specification algorithm does, plus some more. Thus, it necessarily covers $\mathcal C$.

\vspace{\baselineskip} This algorithm is illustrated with the same vowel system that we have been using throughout, i.e. where $C$ includes the front vowels, the high vowels, the round vowels, and all singletons.

\begin{figure}[h]
\includegraphics[width=0.9\textwidth]{vowelHarmony_contrastive.png}
\caption{Class system and featurization yielded by contrastive specification}
\label{fig:contrastive}
\end{figure}

\noindent Note that the feature system yielded by contrastive specification is much more expressive than the one yielded by privative specification. However, it is still not maximally expressive, since it still contains `$0$' values. Zero values are assigned to daughters-of-daughters. For example, suppose that stridents are daughters of coronals, and coronals are daughters of $\Sigma$. Then contrastive specification will create a \phonfeat{-coronal} class (all noncoronals) and a \phonfeat{-strident} class; the latter class will include all coronal nonstridents (but will not include, e.g. labials). Thus, while the \textit{coronal} feature assigns a `$+$' or `$-$' value to every segment, the \textit{strident} feature assigns a `$0$' value to noncoronals. If it is desired to eliminate all `$0$' values, one can do complementation with respect to $\Sigma$ rather than the single parent. Indeed, that is the final variant we discuss -- full specification.

\section{Full specification}

Full specification differs from contrastive specification in that complementation is calculated with respect to the whole alphabet, rather than the parent class. Therefore, it is algorithmically almost the same as contrastive specification.

\vspace{\baselineskip} \noindent \begin{algorithmic}
    \REQUIRE $\mathcal C_\cap$ is the intersectional closure of input class system $(\mathcal C, \Sigma)$
    \ENSURE $\mathcal F$ is a featurization over $\mathcal V = \{ +, - \}$ which covers $\mathcal C$
    \STATE
    \STATE $\mathcal Q \leftarrow \mathcal C_\cap$
    \STATE $\mathcal F \leftarrow \varnothing$
    \STATE
    \WHILE{$\mathcal Q \neq \varnothing$}
        \STATE $X \leftarrow \textsc{dequeue}(\mathcal Q)$
        \IF{$| \textsc{parents}(X) | = 1$}
            \STATE define $f_X : \Sigma \rightarrow \mathcal V$ by $f_X (\sigma) = \begin{cases}
                    + & \text{if } \sigma \in X \\
                    - & \text{otherwise}
                    \end{cases}$
            \STATE $\textsc{append}(\mathcal F, f_X)$
            \STATE
            \STATE $\mathcal C_\cap \leftarrow \textsc{IntersectionalClosure}(\mathcal C_\cap, \mathcal Q' = \{\Sigma \setminus X\})$
        \ENDIF
    \ENDWHILE
\end{algorithmic}

\vspace{\baselineskip} \noindent The full specification algorithm is sound for the same reason that the contrastive specification algorithm is -- it considers a superset of classes that the privative specification algorithm does, and thus it covers the input.

\vspace{\baselineskip} The key way in which full specification differs from contrastive specification is that no privative specification can occur whatsoever. For example, if a single feature \phonfeat{+nasal} is used to pick out nasal segments, then the feature system will also generate the class \phonfeat{-nasal} consisting of all non-nasal segments. According to our understanding of nasal typology, this is probably not the desired behavior for the nasal feature. Fortunately, it can be avoided by ensuring that the nasals are generated by pre-existing features rather than needing their own feature. For example, if \phonfeat{-continuant} picks out the nasals and oral stops, while \phonfeat{+sonorant} picks out vowels, glides, liquids, and nasals, then the nasal class is picked out by \phonfeat{-continuant \\ +sonorant}. Therefore, the set of all non-nasals is not generated as a complement class (although, the set of continuant nonsonorants is, as well as the set of continuant sonorants and the set of noncontinuant nonsonorants).

\section{Discussion}

In this paper, we have given a number of algorithms which assign a featurization to a set of classes, such that every class in the input can be picked out by a featural description. We gave several variants of the algorithm, differing in how conservative they are with respect to the input. The most conservative algorithm assigns a privative specification, i.e. feature functions which only pick out positively specified elements. Contrastive underspecification is achieved with the same algorithm, except that a negative specification is assigned just in case the complement of a class (with respect to the parent class) is in the input. Contrastive specification is similar, except that a negative specification is assigned even if the complement (with respect to the parent) was not in the input. Full specification is similar to contrastive specification, except the complement is taken with respect to the entire segmental alphabet. In this section, we discuss some outstanding issues, such as feature economy, how the current work bears on feature theory, and application toward a richer theory of feature learning.

\subsection{Feature economy and expressivity}

Here we present some examples which illustrate a little more about the expressiveness of class systems, and the relation between algorithm conservativeness and expressiveness.

Let $\mathcal C = \{ \{\sigma\} \, | \, \sigma \in \Sigma \}$; that is, the input consists of all and only the singleton sets. For convenience, we will refer to this as the \textit{singleton input}. Consider what happens when the privative specification algorithm is run on the singleton input. It will yield a featurization with $N$ features, where $N$ is the cardinality of $\Sigma$, and this is because each segment gets its own feature. This featurization will only generate the classes in the input (and $\Sigma$, and $\varnothing$).

The opposite extreme is obtained by the \textit{singleton complement} input -- where the input consists not of all singleton sets, but the complement of each singleton set:  $\mathcal C = \{ \Sigma \setminus \{\sigma\} \, | \, \sigma \in \Sigma \}$. It is left as an exercise for the reader to show that when the privative specification algorithm is given this input, it generates the full powerset of $\Sigma$ -- every possible subset gets a unique combination of features. Thus, privative specification is still compatible with a maximally expressive system.

The powerset of $\Sigma$ is also generated by running the full specification algorithm on the singleton input. Thus, there are cases a more conservative algorithm yields the same class system as a less conservative algorithm. In fact, it is generally true that the more conservative algorithms can achieve the same level of expressiveness as any less conservative algorithm, by virtue of including the relevant complement classes in the input. For example, if all complement classes (with respect to $\Sigma$ are included, the privative specification algorithm yields the same class system as the full specification one does (the singleton input discussed above is a special case of this). Moreover, contrastive underspecification, contrastive specification, and full specification all yield the same featurization (as well as the same class system) if every relevant complement class is included. In short, the algorithms can yield radically different class systems depending on their input -- but all can be made highly expressive by tailoring the input appropriately.

\subsection{Relation to feature theory}

As the examples in the preceding section illustrate, the most conservative algorithms (privative specification, contrastive underspecification) are able to yield class systems that are as expressive as the less conservative algorithms. However, the converse is not true. For example, full specification cannot yield a class system as unexpressive as the singleton input does under privative specification. We regard this kind of question as an interesting area for future work; but as working phonologists, we are also concerned with the question -- what is the best algorithm to use? Put another way, what matters for feature systems? One principle is that a feature system is good to the extent that learned features render the grammar simpler and/or more insightful. For example, the use of `$+$' and `$-$' values yields insight if both values behave the same with respect to a harmony or assimilation process.

We do not at present have strong feelings regarding what a feature system \textit{should} do. But we have something else -- the received wisdom of the field. Our sense is that modern phonologists generally \begin{itemize}
    \item treat certain features as binary, e.g. all segments are either \phonfeat{+son} or \phonfeat{-son}
    \item treat certain features as privative, e.g. nasals are \phonfeat{+nasal} and all others are \phonfeat{0nasal}
    \item treat most features as ternary, e.g. all obstruents are \phonfeat{+voiced} or \phonfeat{-voiced}, but sonorants are simply \phonfeat{0voiced}
    \end{itemize}
Out of the algorithms we have discussed here, only the contrastive underspecification algorithm is capable of yielding a featurization which creates all 3 feature types. We briefly discuss the conditions necessary for assigning each type of feature under the contrastive underspecification algorithm, and then illustrate with a `simple' example, which is nonetheless more complex than what we have introduced before. \begin{itemize}
    \item binary features are generated when a class $X$ and its complement $\Sigma \setminus X$ are both in the input
    \item privative features are generated when a class $X$ is in the input, but no complement (with respect to any ancestor, including its parent, $\Sigma$, and any intervening classes) is
    \item ternary features are generated when a class $X$ is in the input, and its complement $\overline{X}$ with respect to some ancestor other than $\Sigma$ is in the input
    \end{itemize}

\vspace{\baselineskip} With these points in hand, we present an example which generates privative, binary, and ternary features. Let $\mathcal C$ include the following: \begin{itemize}
    \item \textit{inventory} -- \{a, i, u, l, r, m, n, \textipa{N}, p, t, k, b, d, g\}
    \item \textit{consonants} -- \{l, r, m, n, \textipa{N}, p, t, k, b, d, g\}
    \item \textit{sonorants} -- \{a, i, u, l, r, m, n, \textipa{N}\}
    \item \textit{obstruents} -- \{p, t, k, b, d, g\}
    \item \textit{coronal} -- \{n, l, r, t, d\}
    \item \textit{vowels} -- \{a, i, u\}
    \item \textit{nasals} -- \{m, n, \textipa{N}\}
    \item \textit{voiceless} -- \{p, t, k\}
    \item \textit{voiced} -- \{b, d, g\}
    \item \textit{labial} -- \{m, p, b\}
    \item \textit{dorsal} -- \{\textipa{N}, k, g\}
    \item \textit{liquids} -- \{l, r\}
    \item \textit{lateral} -- \{l\}
    \item \textit{rhotic} -- \{r\}
    \end{itemize}
The class system that results from running the contrastive underspecification algorithm on this input is shown in Fig.~\ref{fig:under}. The features \phonfeat{cons} and \phonfeat{son} are binary because each one partitions $\Sigma$. The features \phonfeat{LAB}, \phonfeat{COR}, \phonfeat{DOR}, \phonfeat{nas} and \phonfeat{liquid} are privative, because their complement (with respect to every ancestor) is not included in the input. The remaining features \phonfeat{vcd} and \phonfeat{lat} are ternary, because their complements (with respect to the parent, which is not $\Sigma$) are included in the input.

TODO: make that fig!

We leave it as an exercise to the reader to investigate what happens to the `voicing' feature if the input includes the class of all phonetically voiced segments (i.e. $\Sigma \setminus \text{\{p, t, k\}}$).

A final consideration associated with feature theory is that the features be learnable. There exist various proposals as to how features might be learned, e.g. from acoustic data \cite{TODO}, from articulatory data \cite{TODO}, or from distributional statistics \cite{TODO}. However, every proposal that has been fleshed out enough to be tested has proven inadequate. It seems likely to us that progress will come from integrating multiple sources of information. In the next section, we sketch how the algorithms described in this paper might be integrated with phonetic grounding, and/or distributional learning to learn features.

\subsection{Feature learning}

TODO: [5] some kind of Bayesian thing like what Connor was working on originally -- phonetics and distributional learning propose, grammar disposes?


\bibliography{mybib.bib}
\bibliographystyle{apacite}

\end{document}
